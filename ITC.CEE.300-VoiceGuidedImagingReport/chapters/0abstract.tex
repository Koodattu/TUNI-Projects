This report describes the design and implementation of a voice-guided imaging application that integrates multiple generative AI models for speech transcription, intent recognition, and multimedia generation. The project addresses the growing demand for voice-driven interactions in human-computer interfaces and explores the feasibility of deploying open-source AI models locally, emphasizing privacy, resource optimization, and multi-modal integration.

The system combines speech-to-text transcription and translation, intent recognition via a language model, and diffusion-based models for image and video generation. Key methods include dynamic model loading, memory-efficient quantization, and CPU offloading, enabling deployment on consumer-grade hardware. Additional optimizations, such as partial updates and progress indicators, ensure responsiveness during computationally intensive tasks.

The evaluation highlights the applicationâ€™s ability to handle real-time interactions while balancing speed and accuracy. Demonstration scenarios showcase image creation, editing, and video generation using natural language voice commands. The findings underscore the potential of locally hosted, multi-model AI systems, providing a foundation for future research into scalable and privacy-focused AI applications.