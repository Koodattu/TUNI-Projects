\chapter{Results, Demonstrations and Considerations}
This chapter evaluates the performance and functionality of the voice-guided imaging application. It includes an overview of the user interface, detailed demonstration scenarios, performance observations, and key considerations regarding the system's limitations and areas for improvement.

\section{User Interface Overview}
The image \ref{fig:app-ui} below showcases the implemented user interface. The features are explained after the image, going through the user interface elements in the image from top to bottom, right to left.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth,height=7.5cm,keepaspectratio]{images/app-ui.PNG}
\caption[Short Caption]{Image of the application's user interface.}
\label{fig:app-ui}
\end{figure}

\begin{itemize}
    \item \textbf{Microphone Status:} Displays the current recording status: muted, recording or processing. The status element is reactive and color coded to provide feedback to the user. It also provides instructions based on the settings and the status of the system.
    \item \textbf{Microphone Behavior:} Choice between push-to-talk which requires holding down spacebar to record or completely voice acticity detection based. Push-to-talk functionality makes it more clear for the user when the system is recording and when it is not. It also helps against background noise.
    \item \textbf{Language Selection:} Spoken language can be automatically detected, but can be set for better accuracy. When selecting English, the translation step is omitted and hidden from the interface.
    \item \textbf{Transcription:} Live transcription when recording. The audio is constantly sent to backend for transcription and shown here.
    \item \textbf{Translation:} Full translation is shown here when the user is done giving the voice command. Not shown when selecting English as input language.
    \item \textbf{Inferred Action:} The action which will be executed according to inferred user intent and the prompt which will be fed to the diffusion models.
    \item \textbf{System Status Message:} Shows what the system is currently doing. System status, system messages, image generation steps are shown here for feedback.
    \item \textbf{Chart:} Displays all the steps and prompts for the current image in a flowchart style view.
    \item \textbf{Current Image:} The most recent image is shown here. The starting image is a blank canvas.
    \item \textbf{Gallery:} View all generated images.
\end{itemize}

\section{Demonstration Scenarios}
In addition to the scenario descriptions and images below, a video showcasing all the features of the application is available on YouTube in both \href{https://www.youtube.com/watch?v=BWQ_2Kj094I}{\textcolor{blue}{\underline{English}}} and \href{https://www.youtube.com/watch?v=4OqIeZc3LE0}{\textcolor{blue}{\underline{Finnish}}}, which is most likely a better experience. The demonstration scenarios below go over transcription, translation and intention recognition. We showcase image generation, image editing, reverting an action, re-editing a generated image and editing an already edited image. The videos also showcase the video generation from a base image.

\subsection{Scenario 1: Creating an image}
In this first scenario, the user requested the creation of a new image of the Eiffel Tower using a voice command in Finnish. The first image \ref{fig:first-llm} illustrates the transcription of the voice command by Whisper, its translation into English, and the inferred action determined by Mistral. Whisper accurately transcribed and translated the input, while Mistral correctly inferred the intent to "create a new image of the Eiffel Tower."
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/first-llm.png}
\caption[Short Caption]{First transcription, translation and inferred action.}
\label{fig:first-llm}
\end{figure}
The second image \ref{fig:first-diff} showcases the visual output generated by the system. On the left, we see the initial blank canvas, and on the right, the generated image of the Eiffel Tower. In this process, the image generation model was dynamically loaded into memory to produce the requested result.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/first-diff.png}
\caption[Short Caption]{First step result.}
\label{fig:first-diff}
\end{figure}

\subsection{Scenario 2: Editing an image}
In this second scenario, the user requested to edit the previously generated image of the Eiffel Tower to appear as though it was nighttime. The first image \ref{fig:second-llm} shows the transcription of the voice command in Finnish, its translation into English, and the inferred action determined by Mistral. Whisper accurately transcribed and translated the command, while Mistral correctly inferred the intent to edit the image and adjusted the prompt accordingly.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/second-llm.png}
\caption[Short Caption]{Second transcription, translation and inferred action.}
\label{fig:second-llm}
\end{figure}
The second image \ref{fig:second-diff} highlights the results of the editing process. On the left, we see the original daytime image of the Eiffel Tower, and on the right, the modified nighttime version. To achieve this, the image generation model was unloaded from memory, and the image editing model was dynamically loaded to process the user’s request.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/second-diff.png}
\caption[Short Caption]{Second step result.}
\label{fig:second-diff}
\end{figure}

\subsection{Scenario 3: Reverting an action}
In this third scenario, the user requested the system to revert to the previously generated image. The first image \ref{fig:third-llm} demonstrates how Whisper transcribed the Finnish voice command and translated it into English, followed by Mistral's correct inference of the action to "undo" or revert the most recent change. This capability is crucial when the generated result does not meet the user’s expectations.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/third-llm.png}
\caption[Short Caption]{Third transcription, translation and inferred action.}
\label{fig:third-llm}
\end{figure}
The second image \ref{fig:third-diff} illustrates the outcome of the reversion. On the right, the nighttime image of the Eiffel Tower has been reverted to the original daytime version on the left. This demonstrates the system’s ability to quickly restore previous states, providing users with flexibility and control over their edits.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/third-diff.png}
\caption[Short Caption]{Third step result.}
\label{fig:third-diff}
\end{figure}

\subsection{Scenario 4: Re-editing an image}
In this fourth scenario, we demonstrate the system's ability to handle free-form voice commands when re-editing the original image. The user asked, “What if it was winter in the picture?” instead of explicitly requesting an image edit. The first image \ref{fig:fourth-llm} shows how Whisper transcribed the Finnish command and translated it into English, followed by Mistral's accurate inference of the action to "edit" with a prompt adjustment to change the season to winter.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/fourth-llm.png}
\caption[Short Caption]{Fourth transcription, translation and inferred action.}
\label{fig:fourth-llm}
\end{figure}
The second image \ref{fig:fourth-diff} illustrates the result of this re-editing process. On the left is the original daytime image of the Eiffel Tower, while on the right is the re-edited version, depicting a snowy winter scene. The editing operation was processed quickly, as the image editing model remained loaded in memory from the previous operation.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/fourth-diff.png}
\caption[Short Caption]{Fourth step result.}
\label{fig:fourth-diff}
\end{figure}

\subsection{Scenario 5: Editing an edited image}
In this fifth and final scenario, the user requested to enhance an already edited image by adding fireworks to the sky. The first image \ref{fig:fifth-llm} illustrates Whisper's transcription of the Finnish command, its translation into English, and Mistral's accurate inference to "edit: add fireworks to the image." This showcases the system's ability to iteratively build upon previous edits.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/fifth-llm.png}
\caption[Short Caption]{Fifth transcription, translation and inferred action.}
\label{fig:fifth-llm}
\end{figure}
The second image \ref{fig:fifth-diff} presents the visual transformation. The original edited winter image is on the left, and the updated version with vibrant fireworks lighting up the sky is on the right. This highlights the system’s capability to incrementally enhance images while retaining the contextual elements from previous modifications.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/fifth-diff.png}
\caption[Short Caption]{Fifth step result.}
\label{fig:fifth-diff}
\end{figure}

\subsection{Chart for the current image}
In the following image \ref{fig:chart} is the chart view of all the steps we took in the demonstration scenarios. It displays the images and the prompts used in a flowchart timeline style chart. From this view the user can also pick any of the images for further edits.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/chart.PNG}
\caption[Short Caption]{Chart view displaying prompts and steps for the image.}
\label{fig:chart}
\end{figure}

\section{Performance Observations}
This section presents performance metrics, including latency and memory usage for different tasks such as transcription, intent recognition, and multimedia generation. It highlights the system’s ability to operate efficiently on consumer-grade hardware while managing resource constraints.

\subsection{Latency}
The latency table \ref{tab:latency} below highlights the response times for various tasks performed by the system. Speech transcription and LLM inference exhibit low latency, with times of 500 milliseconds and 200 milliseconds respectively for near real-time interaction. Image and video-related tasks, such as generation and editing, show higher latency, with video generation taking a noticeably long time. These observations indicate that while the system is well-suited for quick tasks like transcription and intent recognition, longer processing times for image and video tasks may require additional feedback mechanisms to maintain a responsive user experience.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Task}              & \textbf{Latency}      \\ \hline
Speech Transcription       & 500 milliseconds     \\ \hline
LLM Inference              & 200 milliseconds     \\ \hline
Image Generation           & 3 seconds           \\ \hline
Image Editing              & 2 seconds           \\ \hline
Video Generation           & 23 seconds          \\ \hline
\end{tabular}
\caption{Latency for Different Tasks}
\label{tab:latency}
\end{table}

\subsection{Memory Usage}
The memory usage table \ref{tab:memory_usage} below outlines the VRAM and RAM requirements for the models used in the application. Lightweight models like Whisper and Mistral require 2 GB and 6.3 GB of VRAM, respectively, making them manageable for most consumer-grade GPUs. However, diffusion-based models like SDXL-Lightning and Stable Video Diffusion demand significantly higher resources, with VRAM usage exceeding 10 GB and RAM usage reaching up to 6.8 GB. These requirements highlight the need for optimization techniques, such as dynamic model loading and memory-efficient inference, to ensure that multiple models can operate concurrently on a single system without overwhelming hardware limitations.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model}                & \textbf{VRAM Usage} & \textbf{RAM Usage} \\ \hline
Whisper Large V3 Turbo        & 2 GB                & N/A                \\ \hline
Mistral 7B Q4                 & 6.3 GB              & N/A                \\ \hline
SDXL-Lightning                & 10.8 GB             & 6.8 GB             \\ \hline
Instruct-Pix2Pix              & 4.9 GB              & 3.2 GB             \\ \hline
Stable Video Diffusion        & 12.6 GB             & 4.2 GB             \\ \hline
\end{tabular}
\caption{Memory Usage for Different Models}
\label{tab:memory_usage}
\end{table}

\section{Considerations}
This section discusses challenges encountered during the project, such as issues with speech recognition, ambiguous user commands, and the single-user focus of the current implementation. Recommendations for addressing these challenges in future iterations are also outlined.

\subsection{Speech Recognition Challenges}  
Speech transcription and translation accuracy can be affected by factors such as background noise, strong accents, fast speech, or low-quality microphones. These issues may lead to errors in transcriptions, impacting downstream tasks like intent recognition and command execution. Users can mitigate these problems by speaking clearly at a moderate pace, and using higher-quality microphones to improve audio input. For places with a lot of background noise, using a directional microphone can help. 

\subsection{Ambiguous User Commands}  
Understanding user intent relies heavily on clear and specific instructions. Commands like "make it better" or "combine two images" lack the detail required for the system to generate actionable results. While the application currently displays errors for unsupported or unclear commands, future enhancements could include prompts for clarification or suggestions to guide users toward valid inputs. However, as this system is designed as a proof of concept rather than an end-user product, the focus remains on demonstrating core functionality rather than usability, user experience and error handling.  

\subsection{Single-User Focus}  
The current implementation is made for single-user interactions and does not support multiple users. Handling concurrent requests would require implementing sessions and processing queues. The system would slow down fast with multiple users sending requests at the same time. Future improvements could explore using cloud AI services to scale performance for multi-user scenarios.