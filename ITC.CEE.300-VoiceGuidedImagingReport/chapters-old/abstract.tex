The Voice-Guided Imaging project explores the development of AI-powered applications that integrate multiple self-hosted generative AI models. Created as part of the GPT-Lab Sein√§joki initiative, the project serves as a practical case study for building modular AI systems with a focus on intention recognition. It demonstrates how voice commands can control image and video generation tasks by combining open-source models and tools.

The application features OpenAI Whisper for speech-to-text transcription, Mistral Instruct for intent recognition, and diffusion-based models like SDXL-Lightning and Instruct-Pix2Pix for image creation and editing. Stable Video Diffusion is also used to generate videos. These models run locally, providing flexibility, privacy, and independence from external services.

Key design goals included optimizing performance for low latency, effectively managing GPU resources, and enabling real-time communication. The project highlights the challenges and benefits of using locally hosted models, such as configuration complexity, scalability limitations, and resource constraints.

This report documents the technical implementation of the application, focusing on modular design, model integration, and performance insights. It also reflects on lessons learned and proposes directions for future work, including comparisons with proprietary AI services and broader evaluations of cost, latency, and scalability.